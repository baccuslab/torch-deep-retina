{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchdeepretina.datas import loadexpt, _loadexpt_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('/home/salamander/experiments/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hs(model, batch_size, device):\n",
    "    hs = []\n",
    "    hs.append(torch.zeros(batch_size, *model.h_shapes[0]).to(device))\n",
    "    hs[0][:,0] = 1\n",
    "    hs.append(deque([],maxlen=model.seq_len))\n",
    "    for i in range(model.seq_len):\n",
    "        hs[1].append(torch.zeros(batch_size, *model.h_shapes[1]).to(device))\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = get_hs(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "class BatchRnnSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, length, batch_size, seq_len):\n",
    "        self.length = length\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_idx = 0\n",
    "        count = 0\n",
    "        while batch_idx < self.length // self.batch_size:\n",
    "            batch = [batch_idx + n * self.__len__() for n in range(self.batch_size)]\n",
    "            yield batch\n",
    "            batch_idx += 1\n",
    "            count += 1\n",
    "            if count == self.seq_len:\n",
    "                count = 0\n",
    "                batch_idx -= (self.seq_len - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.length // self.batch_size - self.seq_len + 1) * self.seq_len + self.seq_len -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BatchRnnSampler(length=30, batch_size=3, seq_len=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BatchRnnSampler(length=30, batch_size=3, seq_len=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadexpt('15-10-07', [0,1,2,3,4], 'naturalscene', 'train', 40, 0, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359762, 40, 50, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359762, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinetic.data import BatchRnnSampler, TrainDataset, ValidationDataset\n",
    "dataset = TrainDataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_sampler = BatchRnnSampler(length=len(dataset), batch_size=512, seq_len=8)\n",
    "data = DataLoader(dataset=dataset, batch_sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(x,y) in enumerate(data):\n",
    "    if i == 0:\n",
    "        inpt = x\n",
    "        trgt = y\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 40, 50, 50])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0035025242913605073\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "device = torch.device('cuda:1')\n",
    "hs = get_hs(model, 1, device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pearsons = []\n",
    "    val_pred = []\n",
    "    val_targ = []\n",
    "    for x,y in data:\n",
    "        x = x.to(device)\n",
    "        out, hs = model(x, hs)\n",
    "        val_pred.append(out.detach().cpu().numpy().squeeze())\n",
    "        val_targ.append(y.detach().numpy().squeeze())\n",
    "    val_pred = np.stack(val_pred, axis=0)\n",
    "    val_targ = np.stack(val_targ, axis=0)\n",
    "    for cell in range(5):\n",
    "        pearsons.append(pearsonr(val_pred[:,cell],val_targ[:,cell])[0])\n",
    "    print(np.array(pearsons).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadexpt('15-10-07', [0,1,2,3,4], 'naturalscene', 'train',\n",
    "                40, 0, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.X\n",
    "batch_size = 512\n",
    "length = X.shape[0]//batch_size\n",
    "tot_len = length*batch_size\n",
    "X_trunc = X[:tot_len].reshape(batch_size, length, *X.shape[1:])\n",
    "X_trunc = np.transpose(X_trunc, (1,0,2,3,4))\n",
    "y = data.y\n",
    "batch_size = 512\n",
    "length = y.shape[0]//batch_size\n",
    "tot_len = length*batch_size\n",
    "y_trunc = y[:tot_len].reshape(batch_size, length, *y.shape[1:])\n",
    "y_trunc = np.transpose(y_trunc, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702, 512, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trunc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "nn.Parameter(torch.ones((5,2)).float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './kinetic')\n",
    "from kinetic.models import KineticsChannelModel\n",
    "from kinetic.evaluation import pearsonr_eval\n",
    "from  torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "device = torch.device('cuda:1')\n",
    "model = KineticsChannelModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "checkpoint_path = '/home/xhding/saved_model/channel/epoch_45_loss_-3.0704265886546778_pearson_0.41805463828607137.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['model_state_dict'].pop(\"bipolar.2.running_mean\")\n",
    "checkpoint['model_state_dict'].pop(\"bipolar.2.running_var\")\n",
    "checkpoint['model_state_dict'].pop(\"bipolar.2.num_batches_tracked\")\n",
    "checkpoint['model_state_dict'].pop(\"amacrine.3.running_mean\")\n",
    "checkpoint['model_state_dict'].pop(\"amacrine.3.running_var\")\n",
    "checkpoint['model_state_dict'].pop(\"amacrine.3.num_batches_tracked\")\n",
    "checkpoint['model_state_dict'].pop(\"ganglion.1.running_mean\")\n",
    "checkpoint['model_state_dict'].pop(\"ganglion.1.running_var\")\n",
    "checkpoint['model_state_dict'].pop(\"ganglion.1.num_batches_tracked\")\n",
    "torch.save(checkpoint, '/home/xhding/saved_model/channel/epoch_46.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['bipolar.0.convs.0.weight', 'bipolar.0.convs.1.weight', 'bipolar.0.convs.2.weight', 'bipolar.0.convs.3.weight', 'bipolar.0.convs.4.weight', 'bipolar.0.convs.5.weight', 'bipolar.0.convs.6.weight', 'bipolar.0.convs.6.bias', 'bipolar.2.weight', 'bipolar.2.bias', 'bipolar.3.sigma', 'bipolar.6.multiplier', 'kinetics.ka', 'kinetics.kfi', 'kinetics.kfr', 'kinetics.ksi', 'kinetics.ksr', 'kinet_scale.scale_param', 'kinet_scale.shift_param', 'amacrine.1.convs.0.weight', 'amacrine.1.convs.1.weight', 'amacrine.1.convs.2.weight', 'amacrine.1.convs.3.weight', 'amacrine.1.convs.4.weight', 'amacrine.1.convs.4.bias', 'amacrine.3.weight', 'amacrine.3.bias', 'amacrine.4.sigma', 'ganglion.0.weight', 'ganglion.0.bias', 'ganglion.1.weight', 'ganglion.1.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['bipolar.0.convs.0.weight', 'bipolar.0.convs.1.weight', 'bipolar.0.convs.2.weight', 'bipolar.0.convs.3.weight', 'bipolar.0.convs.4.weight', 'bipolar.0.convs.5.weight', 'bipolar.0.convs.6.weight', 'bipolar.0.convs.6.bias', 'bipolar.2.weight', 'bipolar.2.bias', 'bipolar.3.sigma', 'bipolar.6.multiplier', 'kinetics.ka', 'kinetics.kfi', 'kinetics.kfr', 'kinetics.ksi', 'kinetics.ksr', 'kinet_scale.scale_param', 'kinet_scale.shift_param', 'amacrine.1.convs.0.weight', 'amacrine.1.convs.1.weight', 'amacrine.1.convs.2.weight', 'amacrine.1.convs.3.weight', 'amacrine.1.convs.4.weight', 'amacrine.1.convs.4.bias', 'amacrine.3.weight', 'amacrine.3.bias', 'amacrine.4.sigma', 'ganglion.0.weight', 'ganglion.0.bias', 'ganglion.1.weight', 'ganglion.1.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model_state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        super().__init__()\n",
    "        data = loadexpt('15-10-07', [0,1,2,3,4], 'naturalscene', 'train',\n",
    "                        40, 0, data_path=data_path)\n",
    "        val_size = 30000\n",
    "        self.X = data.X[-val_size:]\n",
    "        self.y = data.y[-val_size:]\n",
    "        self.centers = data.centers\n",
    "        self.stats = data.stats\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inpt = torch.from_numpy(self.X[index])\n",
    "        trgt = torch.from_numpy(self.y[index])\n",
    "        return (inpt, trgt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 10368])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d61bc715c63b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValidationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpearson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpearsonr_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspaces/torch-deep-retina/kinetic/evaluation.py\u001b[0m in \u001b[0;36mpearsonr_eval\u001b[0;34m(model, data, n_units, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mval_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mval_targ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/torch-deep-retina/kinetic/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hs)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mSecond\u001b[0m \u001b[0melement\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdeque\u001b[0m \u001b[0mof\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mover\u001b[0m \u001b[0mpast\u001b[0m \u001b[0mD\u001b[0m \u001b[0mtime\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipolar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkinetics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 10368])"
     ]
    }
   ],
   "source": [
    "validation_data =  DataLoader(ValidationDataset(data_path))\n",
    "pearson = pearsonr_eval(model, validation_data, 5, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3579518903352822"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  210,\n",
       "  220,\n",
       "  230,\n",
       "  240,\n",
       "  250,\n",
       "  260,\n",
       "  270,\n",
       "  280,\n",
       "  290,\n",
       "  300,\n",
       "  310,\n",
       "  320,\n",
       "  330,\n",
       "  340,\n",
       "  350,\n",
       "  360,\n",
       "  370,\n",
       "  380,\n",
       "  390,\n",
       "  400,\n",
       "  410,\n",
       "  420,\n",
       "  430,\n",
       "  440,\n",
       "  450,\n",
       "  460,\n",
       "  470,\n",
       "  480,\n",
       "  490],\n",
       " [1,\n",
       "  11,\n",
       "  21,\n",
       "  31,\n",
       "  41,\n",
       "  51,\n",
       "  61,\n",
       "  71,\n",
       "  81,\n",
       "  91,\n",
       "  101,\n",
       "  111,\n",
       "  121,\n",
       "  131,\n",
       "  141,\n",
       "  151,\n",
       "  161,\n",
       "  171,\n",
       "  181,\n",
       "  191,\n",
       "  201,\n",
       "  211,\n",
       "  221,\n",
       "  231,\n",
       "  241,\n",
       "  251,\n",
       "  261,\n",
       "  271,\n",
       "  281,\n",
       "  291,\n",
       "  301,\n",
       "  311,\n",
       "  321,\n",
       "  331,\n",
       "  341,\n",
       "  351,\n",
       "  361,\n",
       "  371,\n",
       "  381,\n",
       "  391,\n",
       "  401,\n",
       "  411,\n",
       "  421,\n",
       "  431,\n",
       "  441,\n",
       "  451,\n",
       "  461,\n",
       "  471,\n",
       "  481,\n",
       "  491],\n",
       " [2,\n",
       "  12,\n",
       "  22,\n",
       "  32,\n",
       "  42,\n",
       "  52,\n",
       "  62,\n",
       "  72,\n",
       "  82,\n",
       "  92,\n",
       "  102,\n",
       "  112,\n",
       "  122,\n",
       "  132,\n",
       "  142,\n",
       "  152,\n",
       "  162,\n",
       "  172,\n",
       "  182,\n",
       "  192,\n",
       "  202,\n",
       "  212,\n",
       "  222,\n",
       "  232,\n",
       "  242,\n",
       "  252,\n",
       "  262,\n",
       "  272,\n",
       "  282,\n",
       "  292,\n",
       "  302,\n",
       "  312,\n",
       "  322,\n",
       "  332,\n",
       "  342,\n",
       "  352,\n",
       "  362,\n",
       "  372,\n",
       "  382,\n",
       "  392,\n",
       "  402,\n",
       "  412,\n",
       "  422,\n",
       "  432,\n",
       "  442,\n",
       "  452,\n",
       "  462,\n",
       "  472,\n",
       "  482,\n",
       "  492],\n",
       " [3,\n",
       "  13,\n",
       "  23,\n",
       "  33,\n",
       "  43,\n",
       "  53,\n",
       "  63,\n",
       "  73,\n",
       "  83,\n",
       "  93,\n",
       "  103,\n",
       "  113,\n",
       "  123,\n",
       "  133,\n",
       "  143,\n",
       "  153,\n",
       "  163,\n",
       "  173,\n",
       "  183,\n",
       "  193,\n",
       "  203,\n",
       "  213,\n",
       "  223,\n",
       "  233,\n",
       "  243,\n",
       "  253,\n",
       "  263,\n",
       "  273,\n",
       "  283,\n",
       "  293,\n",
       "  303,\n",
       "  313,\n",
       "  323,\n",
       "  333,\n",
       "  343,\n",
       "  353,\n",
       "  363,\n",
       "  373,\n",
       "  383,\n",
       "  393,\n",
       "  403,\n",
       "  413,\n",
       "  423,\n",
       "  433,\n",
       "  443,\n",
       "  453,\n",
       "  463,\n",
       "  473,\n",
       "  483,\n",
       "  493],\n",
       " [4,\n",
       "  14,\n",
       "  24,\n",
       "  34,\n",
       "  44,\n",
       "  54,\n",
       "  64,\n",
       "  74,\n",
       "  84,\n",
       "  94,\n",
       "  104,\n",
       "  114,\n",
       "  124,\n",
       "  134,\n",
       "  144,\n",
       "  154,\n",
       "  164,\n",
       "  174,\n",
       "  184,\n",
       "  194,\n",
       "  204,\n",
       "  214,\n",
       "  224,\n",
       "  234,\n",
       "  244,\n",
       "  254,\n",
       "  264,\n",
       "  274,\n",
       "  284,\n",
       "  294,\n",
       "  304,\n",
       "  314,\n",
       "  324,\n",
       "  334,\n",
       "  344,\n",
       "  354,\n",
       "  364,\n",
       "  374,\n",
       "  384,\n",
       "  394,\n",
       "  404,\n",
       "  414,\n",
       "  424,\n",
       "  434,\n",
       "  444,\n",
       "  454,\n",
       "  464,\n",
       "  474,\n",
       "  484,\n",
       "  494],\n",
       " [5,\n",
       "  15,\n",
       "  25,\n",
       "  35,\n",
       "  45,\n",
       "  55,\n",
       "  65,\n",
       "  75,\n",
       "  85,\n",
       "  95,\n",
       "  105,\n",
       "  115,\n",
       "  125,\n",
       "  135,\n",
       "  145,\n",
       "  155,\n",
       "  165,\n",
       "  175,\n",
       "  185,\n",
       "  195,\n",
       "  205,\n",
       "  215,\n",
       "  225,\n",
       "  235,\n",
       "  245,\n",
       "  255,\n",
       "  265,\n",
       "  275,\n",
       "  285,\n",
       "  295,\n",
       "  305,\n",
       "  315,\n",
       "  325,\n",
       "  335,\n",
       "  345,\n",
       "  355,\n",
       "  365,\n",
       "  375,\n",
       "  385,\n",
       "  395,\n",
       "  405,\n",
       "  415,\n",
       "  425,\n",
       "  435,\n",
       "  445,\n",
       "  455,\n",
       "  465,\n",
       "  475,\n",
       "  485,\n",
       "  495],\n",
       " [6,\n",
       "  16,\n",
       "  26,\n",
       "  36,\n",
       "  46,\n",
       "  56,\n",
       "  66,\n",
       "  76,\n",
       "  86,\n",
       "  96,\n",
       "  106,\n",
       "  116,\n",
       "  126,\n",
       "  136,\n",
       "  146,\n",
       "  156,\n",
       "  166,\n",
       "  176,\n",
       "  186,\n",
       "  196,\n",
       "  206,\n",
       "  216,\n",
       "  226,\n",
       "  236,\n",
       "  246,\n",
       "  256,\n",
       "  266,\n",
       "  276,\n",
       "  286,\n",
       "  296,\n",
       "  306,\n",
       "  316,\n",
       "  326,\n",
       "  336,\n",
       "  346,\n",
       "  356,\n",
       "  366,\n",
       "  376,\n",
       "  386,\n",
       "  396,\n",
       "  406,\n",
       "  416,\n",
       "  426,\n",
       "  436,\n",
       "  446,\n",
       "  456,\n",
       "  466,\n",
       "  476,\n",
       "  486,\n",
       "  496],\n",
       " [7,\n",
       "  17,\n",
       "  27,\n",
       "  37,\n",
       "  47,\n",
       "  57,\n",
       "  67,\n",
       "  77,\n",
       "  87,\n",
       "  97,\n",
       "  107,\n",
       "  117,\n",
       "  127,\n",
       "  137,\n",
       "  147,\n",
       "  157,\n",
       "  167,\n",
       "  177,\n",
       "  187,\n",
       "  197,\n",
       "  207,\n",
       "  217,\n",
       "  227,\n",
       "  237,\n",
       "  247,\n",
       "  257,\n",
       "  267,\n",
       "  277,\n",
       "  287,\n",
       "  297,\n",
       "  307,\n",
       "  317,\n",
       "  327,\n",
       "  337,\n",
       "  347,\n",
       "  357,\n",
       "  367,\n",
       "  377,\n",
       "  387,\n",
       "  397,\n",
       "  407,\n",
       "  417,\n",
       "  427,\n",
       "  437,\n",
       "  447,\n",
       "  457,\n",
       "  467,\n",
       "  477,\n",
       "  487,\n",
       "  497],\n",
       " [1,\n",
       "  11,\n",
       "  21,\n",
       "  31,\n",
       "  41,\n",
       "  51,\n",
       "  61,\n",
       "  71,\n",
       "  81,\n",
       "  91,\n",
       "  101,\n",
       "  111,\n",
       "  121,\n",
       "  131,\n",
       "  141,\n",
       "  151,\n",
       "  161,\n",
       "  171,\n",
       "  181,\n",
       "  191,\n",
       "  201,\n",
       "  211,\n",
       "  221,\n",
       "  231,\n",
       "  241,\n",
       "  251,\n",
       "  261,\n",
       "  271,\n",
       "  281,\n",
       "  291,\n",
       "  301,\n",
       "  311,\n",
       "  321,\n",
       "  331,\n",
       "  341,\n",
       "  351,\n",
       "  361,\n",
       "  371,\n",
       "  381,\n",
       "  391,\n",
       "  401,\n",
       "  411,\n",
       "  421,\n",
       "  431,\n",
       "  441,\n",
       "  451,\n",
       "  461,\n",
       "  471,\n",
       "  481,\n",
       "  491],\n",
       " [2,\n",
       "  12,\n",
       "  22,\n",
       "  32,\n",
       "  42,\n",
       "  52,\n",
       "  62,\n",
       "  72,\n",
       "  82,\n",
       "  92,\n",
       "  102,\n",
       "  112,\n",
       "  122,\n",
       "  132,\n",
       "  142,\n",
       "  152,\n",
       "  162,\n",
       "  172,\n",
       "  182,\n",
       "  192,\n",
       "  202,\n",
       "  212,\n",
       "  222,\n",
       "  232,\n",
       "  242,\n",
       "  252,\n",
       "  262,\n",
       "  272,\n",
       "  282,\n",
       "  292,\n",
       "  302,\n",
       "  312,\n",
       "  322,\n",
       "  332,\n",
       "  342,\n",
       "  352,\n",
       "  362,\n",
       "  372,\n",
       "  382,\n",
       "  392,\n",
       "  402,\n",
       "  412,\n",
       "  422,\n",
       "  432,\n",
       "  442,\n",
       "  452,\n",
       "  462,\n",
       "  472,\n",
       "  482,\n",
       "  492],\n",
       " [3,\n",
       "  13,\n",
       "  23,\n",
       "  33,\n",
       "  43,\n",
       "  53,\n",
       "  63,\n",
       "  73,\n",
       "  83,\n",
       "  93,\n",
       "  103,\n",
       "  113,\n",
       "  123,\n",
       "  133,\n",
       "  143,\n",
       "  153,\n",
       "  163,\n",
       "  173,\n",
       "  183,\n",
       "  193,\n",
       "  203,\n",
       "  213,\n",
       "  223,\n",
       "  233,\n",
       "  243,\n",
       "  253,\n",
       "  263,\n",
       "  273,\n",
       "  283,\n",
       "  293,\n",
       "  303,\n",
       "  313,\n",
       "  323,\n",
       "  333,\n",
       "  343,\n",
       "  353,\n",
       "  363,\n",
       "  373,\n",
       "  383,\n",
       "  393,\n",
       "  403,\n",
       "  413,\n",
       "  423,\n",
       "  433,\n",
       "  443,\n",
       "  453,\n",
       "  463,\n",
       "  473,\n",
       "  483,\n",
       "  493],\n",
       " [4,\n",
       "  14,\n",
       "  24,\n",
       "  34,\n",
       "  44,\n",
       "  54,\n",
       "  64,\n",
       "  74,\n",
       "  84,\n",
       "  94,\n",
       "  104,\n",
       "  114,\n",
       "  124,\n",
       "  134,\n",
       "  144,\n",
       "  154,\n",
       "  164,\n",
       "  174,\n",
       "  184,\n",
       "  194,\n",
       "  204,\n",
       "  214,\n",
       "  224,\n",
       "  234,\n",
       "  244,\n",
       "  254,\n",
       "  264,\n",
       "  274,\n",
       "  284,\n",
       "  294,\n",
       "  304,\n",
       "  314,\n",
       "  324,\n",
       "  334,\n",
       "  344,\n",
       "  354,\n",
       "  364,\n",
       "  374,\n",
       "  384,\n",
       "  394,\n",
       "  404,\n",
       "  414,\n",
       "  424,\n",
       "  434,\n",
       "  444,\n",
       "  454,\n",
       "  464,\n",
       "  474,\n",
       "  484,\n",
       "  494],\n",
       " [5,\n",
       "  15,\n",
       "  25,\n",
       "  35,\n",
       "  45,\n",
       "  55,\n",
       "  65,\n",
       "  75,\n",
       "  85,\n",
       "  95,\n",
       "  105,\n",
       "  115,\n",
       "  125,\n",
       "  135,\n",
       "  145,\n",
       "  155,\n",
       "  165,\n",
       "  175,\n",
       "  185,\n",
       "  195,\n",
       "  205,\n",
       "  215,\n",
       "  225,\n",
       "  235,\n",
       "  245,\n",
       "  255,\n",
       "  265,\n",
       "  275,\n",
       "  285,\n",
       "  295,\n",
       "  305,\n",
       "  315,\n",
       "  325,\n",
       "  335,\n",
       "  345,\n",
       "  355,\n",
       "  365,\n",
       "  375,\n",
       "  385,\n",
       "  395,\n",
       "  405,\n",
       "  415,\n",
       "  425,\n",
       "  435,\n",
       "  445,\n",
       "  455,\n",
       "  465,\n",
       "  475,\n",
       "  485,\n",
       "  495],\n",
       " [6,\n",
       "  16,\n",
       "  26,\n",
       "  36,\n",
       "  46,\n",
       "  56,\n",
       "  66,\n",
       "  76,\n",
       "  86,\n",
       "  96,\n",
       "  106,\n",
       "  116,\n",
       "  126,\n",
       "  136,\n",
       "  146,\n",
       "  156,\n",
       "  166,\n",
       "  176,\n",
       "  186,\n",
       "  196,\n",
       "  206,\n",
       "  216,\n",
       "  226,\n",
       "  236,\n",
       "  246,\n",
       "  256,\n",
       "  266,\n",
       "  276,\n",
       "  286,\n",
       "  296,\n",
       "  306,\n",
       "  316,\n",
       "  326,\n",
       "  336,\n",
       "  346,\n",
       "  356,\n",
       "  366,\n",
       "  376,\n",
       "  386,\n",
       "  396,\n",
       "  406,\n",
       "  416,\n",
       "  426,\n",
       "  436,\n",
       "  446,\n",
       "  456,\n",
       "  466,\n",
       "  476,\n",
       "  486,\n",
       "  496],\n",
       " [7,\n",
       "  17,\n",
       "  27,\n",
       "  37,\n",
       "  47,\n",
       "  57,\n",
       "  67,\n",
       "  77,\n",
       "  87,\n",
       "  97,\n",
       "  107,\n",
       "  117,\n",
       "  127,\n",
       "  137,\n",
       "  147,\n",
       "  157,\n",
       "  167,\n",
       "  177,\n",
       "  187,\n",
       "  197,\n",
       "  207,\n",
       "  217,\n",
       "  227,\n",
       "  237,\n",
       "  247,\n",
       "  257,\n",
       "  267,\n",
       "  277,\n",
       "  287,\n",
       "  297,\n",
       "  307,\n",
       "  317,\n",
       "  327,\n",
       "  337,\n",
       "  347,\n",
       "  357,\n",
       "  367,\n",
       "  377,\n",
       "  387,\n",
       "  397,\n",
       "  407,\n",
       "  417,\n",
       "  427,\n",
       "  437,\n",
       "  447,\n",
       "  457,\n",
       "  467,\n",
       "  477,\n",
       "  487,\n",
       "  497],\n",
       " [8,\n",
       "  18,\n",
       "  28,\n",
       "  38,\n",
       "  48,\n",
       "  58,\n",
       "  68,\n",
       "  78,\n",
       "  88,\n",
       "  98,\n",
       "  108,\n",
       "  118,\n",
       "  128,\n",
       "  138,\n",
       "  148,\n",
       "  158,\n",
       "  168,\n",
       "  178,\n",
       "  188,\n",
       "  198,\n",
       "  208,\n",
       "  218,\n",
       "  228,\n",
       "  238,\n",
       "  248,\n",
       "  258,\n",
       "  268,\n",
       "  278,\n",
       "  288,\n",
       "  298,\n",
       "  308,\n",
       "  318,\n",
       "  328,\n",
       "  338,\n",
       "  348,\n",
       "  358,\n",
       "  368,\n",
       "  378,\n",
       "  388,\n",
       "  398,\n",
       "  408,\n",
       "  418,\n",
       "  428,\n",
       "  438,\n",
       "  448,\n",
       "  458,\n",
       "  468,\n",
       "  478,\n",
       "  488,\n",
       "  498],\n",
       " [2,\n",
       "  12,\n",
       "  22,\n",
       "  32,\n",
       "  42,\n",
       "  52,\n",
       "  62,\n",
       "  72,\n",
       "  82,\n",
       "  92,\n",
       "  102,\n",
       "  112,\n",
       "  122,\n",
       "  132,\n",
       "  142,\n",
       "  152,\n",
       "  162,\n",
       "  172,\n",
       "  182,\n",
       "  192,\n",
       "  202,\n",
       "  212,\n",
       "  222,\n",
       "  232,\n",
       "  242,\n",
       "  252,\n",
       "  262,\n",
       "  272,\n",
       "  282,\n",
       "  292,\n",
       "  302,\n",
       "  312,\n",
       "  322,\n",
       "  332,\n",
       "  342,\n",
       "  352,\n",
       "  362,\n",
       "  372,\n",
       "  382,\n",
       "  392,\n",
       "  402,\n",
       "  412,\n",
       "  422,\n",
       "  432,\n",
       "  442,\n",
       "  452,\n",
       "  462,\n",
       "  472,\n",
       "  482,\n",
       "  492],\n",
       " [3,\n",
       "  13,\n",
       "  23,\n",
       "  33,\n",
       "  43,\n",
       "  53,\n",
       "  63,\n",
       "  73,\n",
       "  83,\n",
       "  93,\n",
       "  103,\n",
       "  113,\n",
       "  123,\n",
       "  133,\n",
       "  143,\n",
       "  153,\n",
       "  163,\n",
       "  173,\n",
       "  183,\n",
       "  193,\n",
       "  203,\n",
       "  213,\n",
       "  223,\n",
       "  233,\n",
       "  243,\n",
       "  253,\n",
       "  263,\n",
       "  273,\n",
       "  283,\n",
       "  293,\n",
       "  303,\n",
       "  313,\n",
       "  323,\n",
       "  333,\n",
       "  343,\n",
       "  353,\n",
       "  363,\n",
       "  373,\n",
       "  383,\n",
       "  393,\n",
       "  403,\n",
       "  413,\n",
       "  423,\n",
       "  433,\n",
       "  443,\n",
       "  453,\n",
       "  463,\n",
       "  473,\n",
       "  483,\n",
       "  493],\n",
       " [4,\n",
       "  14,\n",
       "  24,\n",
       "  34,\n",
       "  44,\n",
       "  54,\n",
       "  64,\n",
       "  74,\n",
       "  84,\n",
       "  94,\n",
       "  104,\n",
       "  114,\n",
       "  124,\n",
       "  134,\n",
       "  144,\n",
       "  154,\n",
       "  164,\n",
       "  174,\n",
       "  184,\n",
       "  194,\n",
       "  204,\n",
       "  214,\n",
       "  224,\n",
       "  234,\n",
       "  244,\n",
       "  254,\n",
       "  264,\n",
       "  274,\n",
       "  284,\n",
       "  294,\n",
       "  304,\n",
       "  314,\n",
       "  324,\n",
       "  334,\n",
       "  344,\n",
       "  354,\n",
       "  364,\n",
       "  374,\n",
       "  384,\n",
       "  394,\n",
       "  404,\n",
       "  414,\n",
       "  424,\n",
       "  434,\n",
       "  444,\n",
       "  454,\n",
       "  464,\n",
       "  474,\n",
       "  484,\n",
       "  494],\n",
       " [5,\n",
       "  15,\n",
       "  25,\n",
       "  35,\n",
       "  45,\n",
       "  55,\n",
       "  65,\n",
       "  75,\n",
       "  85,\n",
       "  95,\n",
       "  105,\n",
       "  115,\n",
       "  125,\n",
       "  135,\n",
       "  145,\n",
       "  155,\n",
       "  165,\n",
       "  175,\n",
       "  185,\n",
       "  195,\n",
       "  205,\n",
       "  215,\n",
       "  225,\n",
       "  235,\n",
       "  245,\n",
       "  255,\n",
       "  265,\n",
       "  275,\n",
       "  285,\n",
       "  295,\n",
       "  305,\n",
       "  315,\n",
       "  325,\n",
       "  335,\n",
       "  345,\n",
       "  355,\n",
       "  365,\n",
       "  375,\n",
       "  385,\n",
       "  395,\n",
       "  405,\n",
       "  415,\n",
       "  425,\n",
       "  435,\n",
       "  445,\n",
       "  455,\n",
       "  465,\n",
       "  475,\n",
       "  485,\n",
       "  495],\n",
       " [6,\n",
       "  16,\n",
       "  26,\n",
       "  36,\n",
       "  46,\n",
       "  56,\n",
       "  66,\n",
       "  76,\n",
       "  86,\n",
       "  96,\n",
       "  106,\n",
       "  116,\n",
       "  126,\n",
       "  136,\n",
       "  146,\n",
       "  156,\n",
       "  166,\n",
       "  176,\n",
       "  186,\n",
       "  196,\n",
       "  206,\n",
       "  216,\n",
       "  226,\n",
       "  236,\n",
       "  246,\n",
       "  256,\n",
       "  266,\n",
       "  276,\n",
       "  286,\n",
       "  296,\n",
       "  306,\n",
       "  316,\n",
       "  326,\n",
       "  336,\n",
       "  346,\n",
       "  356,\n",
       "  366,\n",
       "  376,\n",
       "  386,\n",
       "  396,\n",
       "  406,\n",
       "  416,\n",
       "  426,\n",
       "  436,\n",
       "  446,\n",
       "  456,\n",
       "  466,\n",
       "  476,\n",
       "  486,\n",
       "  496],\n",
       " [7,\n",
       "  17,\n",
       "  27,\n",
       "  37,\n",
       "  47,\n",
       "  57,\n",
       "  67,\n",
       "  77,\n",
       "  87,\n",
       "  97,\n",
       "  107,\n",
       "  117,\n",
       "  127,\n",
       "  137,\n",
       "  147,\n",
       "  157,\n",
       "  167,\n",
       "  177,\n",
       "  187,\n",
       "  197,\n",
       "  207,\n",
       "  217,\n",
       "  227,\n",
       "  237,\n",
       "  247,\n",
       "  257,\n",
       "  267,\n",
       "  277,\n",
       "  287,\n",
       "  297,\n",
       "  307,\n",
       "  317,\n",
       "  327,\n",
       "  337,\n",
       "  347,\n",
       "  357,\n",
       "  367,\n",
       "  377,\n",
       "  387,\n",
       "  397,\n",
       "  407,\n",
       "  417,\n",
       "  427,\n",
       "  437,\n",
       "  447,\n",
       "  457,\n",
       "  467,\n",
       "  477,\n",
       "  487,\n",
       "  497],\n",
       " [8,\n",
       "  18,\n",
       "  28,\n",
       "  38,\n",
       "  48,\n",
       "  58,\n",
       "  68,\n",
       "  78,\n",
       "  88,\n",
       "  98,\n",
       "  108,\n",
       "  118,\n",
       "  128,\n",
       "  138,\n",
       "  148,\n",
       "  158,\n",
       "  168,\n",
       "  178,\n",
       "  188,\n",
       "  198,\n",
       "  208,\n",
       "  218,\n",
       "  228,\n",
       "  238,\n",
       "  248,\n",
       "  258,\n",
       "  268,\n",
       "  278,\n",
       "  288,\n",
       "  298,\n",
       "  308,\n",
       "  318,\n",
       "  328,\n",
       "  338,\n",
       "  348,\n",
       "  358,\n",
       "  368,\n",
       "  378,\n",
       "  388,\n",
       "  398,\n",
       "  408,\n",
       "  418,\n",
       "  428,\n",
       "  438,\n",
       "  448,\n",
       "  458,\n",
       "  468,\n",
       "  478,\n",
       "  488,\n",
       "  498],\n",
       " [9,\n",
       "  19,\n",
       "  29,\n",
       "  39,\n",
       "  49,\n",
       "  59,\n",
       "  69,\n",
       "  79,\n",
       "  89,\n",
       "  99,\n",
       "  109,\n",
       "  119,\n",
       "  129,\n",
       "  139,\n",
       "  149,\n",
       "  159,\n",
       "  169,\n",
       "  179,\n",
       "  189,\n",
       "  199,\n",
       "  209,\n",
       "  219,\n",
       "  229,\n",
       "  239,\n",
       "  249,\n",
       "  259,\n",
       "  269,\n",
       "  279,\n",
       "  289,\n",
       "  299,\n",
       "  309,\n",
       "  319,\n",
       "  329,\n",
       "  339,\n",
       "  349,\n",
       "  359,\n",
       "  369,\n",
       "  379,\n",
       "  389,\n",
       "  399,\n",
       "  409,\n",
       "  419,\n",
       "  429,\n",
       "  439,\n",
       "  449,\n",
       "  459,\n",
       "  469,\n",
       "  479,\n",
       "  489,\n",
       "  499],\n",
       " [3,\n",
       "  13,\n",
       "  23,\n",
       "  33,\n",
       "  43,\n",
       "  53,\n",
       "  63,\n",
       "  73,\n",
       "  83,\n",
       "  93,\n",
       "  103,\n",
       "  113,\n",
       "  123,\n",
       "  133,\n",
       "  143,\n",
       "  153,\n",
       "  163,\n",
       "  173,\n",
       "  183,\n",
       "  193,\n",
       "  203,\n",
       "  213,\n",
       "  223,\n",
       "  233,\n",
       "  243,\n",
       "  253,\n",
       "  263,\n",
       "  273,\n",
       "  283,\n",
       "  293,\n",
       "  303,\n",
       "  313,\n",
       "  323,\n",
       "  333,\n",
       "  343,\n",
       "  353,\n",
       "  363,\n",
       "  373,\n",
       "  383,\n",
       "  393,\n",
       "  403,\n",
       "  413,\n",
       "  423,\n",
       "  433,\n",
       "  443,\n",
       "  453,\n",
       "  463,\n",
       "  473,\n",
       "  483,\n",
       "  493],\n",
       " [4,\n",
       "  14,\n",
       "  24,\n",
       "  34,\n",
       "  44,\n",
       "  54,\n",
       "  64,\n",
       "  74,\n",
       "  84,\n",
       "  94,\n",
       "  104,\n",
       "  114,\n",
       "  124,\n",
       "  134,\n",
       "  144,\n",
       "  154,\n",
       "  164,\n",
       "  174,\n",
       "  184,\n",
       "  194,\n",
       "  204,\n",
       "  214,\n",
       "  224,\n",
       "  234,\n",
       "  244,\n",
       "  254,\n",
       "  264,\n",
       "  274,\n",
       "  284,\n",
       "  294,\n",
       "  304,\n",
       "  314,\n",
       "  324,\n",
       "  334,\n",
       "  344,\n",
       "  354,\n",
       "  364,\n",
       "  374,\n",
       "  384,\n",
       "  394,\n",
       "  404,\n",
       "  414,\n",
       "  424,\n",
       "  434,\n",
       "  444,\n",
       "  454,\n",
       "  464,\n",
       "  474,\n",
       "  484,\n",
       "  494],\n",
       " [5,\n",
       "  15,\n",
       "  25,\n",
       "  35,\n",
       "  45,\n",
       "  55,\n",
       "  65,\n",
       "  75,\n",
       "  85,\n",
       "  95,\n",
       "  105,\n",
       "  115,\n",
       "  125,\n",
       "  135,\n",
       "  145,\n",
       "  155,\n",
       "  165,\n",
       "  175,\n",
       "  185,\n",
       "  195,\n",
       "  205,\n",
       "  215,\n",
       "  225,\n",
       "  235,\n",
       "  245,\n",
       "  255,\n",
       "  265,\n",
       "  275,\n",
       "  285,\n",
       "  295,\n",
       "  305,\n",
       "  315,\n",
       "  325,\n",
       "  335,\n",
       "  345,\n",
       "  355,\n",
       "  365,\n",
       "  375,\n",
       "  385,\n",
       "  395,\n",
       "  405,\n",
       "  415,\n",
       "  425,\n",
       "  435,\n",
       "  445,\n",
       "  455,\n",
       "  465,\n",
       "  475,\n",
       "  485,\n",
       "  495],\n",
       " [6,\n",
       "  16,\n",
       "  26,\n",
       "  36,\n",
       "  46,\n",
       "  56,\n",
       "  66,\n",
       "  76,\n",
       "  86,\n",
       "  96,\n",
       "  106,\n",
       "  116,\n",
       "  126,\n",
       "  136,\n",
       "  146,\n",
       "  156,\n",
       "  166,\n",
       "  176,\n",
       "  186,\n",
       "  196,\n",
       "  206,\n",
       "  216,\n",
       "  226,\n",
       "  236,\n",
       "  246,\n",
       "  256,\n",
       "  266,\n",
       "  276,\n",
       "  286,\n",
       "  296,\n",
       "  306,\n",
       "  316,\n",
       "  326,\n",
       "  336,\n",
       "  346,\n",
       "  356,\n",
       "  366,\n",
       "  376,\n",
       "  386,\n",
       "  396,\n",
       "  406,\n",
       "  416,\n",
       "  426,\n",
       "  436,\n",
       "  446,\n",
       "  456,\n",
       "  466,\n",
       "  476,\n",
       "  486,\n",
       "  496],\n",
       " [7,\n",
       "  17,\n",
       "  27,\n",
       "  37,\n",
       "  47,\n",
       "  57,\n",
       "  67,\n",
       "  77,\n",
       "  87,\n",
       "  97,\n",
       "  107,\n",
       "  117,\n",
       "  127,\n",
       "  137,\n",
       "  147,\n",
       "  157,\n",
       "  167,\n",
       "  177,\n",
       "  187,\n",
       "  197,\n",
       "  207,\n",
       "  217,\n",
       "  227,\n",
       "  237,\n",
       "  247,\n",
       "  257,\n",
       "  267,\n",
       "  277,\n",
       "  287,\n",
       "  297,\n",
       "  307,\n",
       "  317,\n",
       "  327,\n",
       "  337,\n",
       "  347,\n",
       "  357,\n",
       "  367,\n",
       "  377,\n",
       "  387,\n",
       "  397,\n",
       "  407,\n",
       "  417,\n",
       "  427,\n",
       "  437,\n",
       "  447,\n",
       "  457,\n",
       "  467,\n",
       "  477,\n",
       "  487,\n",
       "  497],\n",
       " [8,\n",
       "  18,\n",
       "  28,\n",
       "  38,\n",
       "  48,\n",
       "  58,\n",
       "  68,\n",
       "  78,\n",
       "  88,\n",
       "  98,\n",
       "  108,\n",
       "  118,\n",
       "  128,\n",
       "  138,\n",
       "  148,\n",
       "  158,\n",
       "  168,\n",
       "  178,\n",
       "  188,\n",
       "  198,\n",
       "  208,\n",
       "  218,\n",
       "  228,\n",
       "  238,\n",
       "  248,\n",
       "  258,\n",
       "  268,\n",
       "  278,\n",
       "  288,\n",
       "  298,\n",
       "  308,\n",
       "  318,\n",
       "  328,\n",
       "  338,\n",
       "  348,\n",
       "  358,\n",
       "  368,\n",
       "  378,\n",
       "  388,\n",
       "  398,\n",
       "  408,\n",
       "  418,\n",
       "  428,\n",
       "  438,\n",
       "  448,\n",
       "  458,\n",
       "  468,\n",
       "  478,\n",
       "  488,\n",
       "  498],\n",
       " [9,\n",
       "  19,\n",
       "  29,\n",
       "  39,\n",
       "  49,\n",
       "  59,\n",
       "  69,\n",
       "  79,\n",
       "  89,\n",
       "  99,\n",
       "  109,\n",
       "  119,\n",
       "  129,\n",
       "  139,\n",
       "  149,\n",
       "  159,\n",
       "  169,\n",
       "  179,\n",
       "  189,\n",
       "  199,\n",
       "  209,\n",
       "  219,\n",
       "  229,\n",
       "  239,\n",
       "  249,\n",
       "  259,\n",
       "  269,\n",
       "  279,\n",
       "  289,\n",
       "  299,\n",
       "  309,\n",
       "  319,\n",
       "  329,\n",
       "  339,\n",
       "  349,\n",
       "  359,\n",
       "  369,\n",
       "  379,\n",
       "  389,\n",
       "  399,\n",
       "  409,\n",
       "  419,\n",
       "  429,\n",
       "  439,\n",
       "  449,\n",
       "  459,\n",
       "  469,\n",
       "  479,\n",
       "  489,\n",
       "  499]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kinetic.data import BatchRnnSampler\n",
    "list(BatchRnnSampler(500, 50, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, 5, 0.1)\n",
    "y = np.sin(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './kinetic')\n",
    "from kinetic.models import KineticsChannelModel\n",
    "from kinetic.evaluation import pearsonr_eval\n",
    "from  torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "model = KineticsChannelModel().to(device)\n",
    "#checkpoint_path = '/home/xhding/saved_model/channel2/epoch_005_loss_-3.28_pearson_-0.0079.pth'\n",
    "checkpoint_path = '/home/xhding/saved_model/channel/epoch_45_loss_-3.0704265886546778_pearson_0.41805463828607137.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        super().__init__()\n",
    "        data = loadexpt('15-10-07', [0,1,2,3,4], 'naturalscene', 'train',\n",
    "                        40, 0, data_path=data_path)\n",
    "        val_size = 30000\n",
    "        self.X = data.X[0:val_size]\n",
    "        self.y = data.y[0:val_size]\n",
    "        self.centers = data.centers\n",
    "        self.stats = data.stats\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inpt = torch.from_numpy(self.X[index])\n",
    "        trgt = torch.from_numpy(self.y[index])\n",
    "        return (inpt, trgt)\n",
    "validation_data =  DataLoader(ValidationDataset(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr_eval(model, data, n_units, device):\n",
    "    hs = get_hs(model, 1, device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pearsons = []\n",
    "        val_pred = []\n",
    "        val_targ = []\n",
    "        for x,y in data:\n",
    "            x = x.to(device)\n",
    "            out, hs = model(x, hs)\n",
    "            val_pred.append(out.detach().cpu().numpy().squeeze())\n",
    "            val_targ.append(y.detach().numpy().squeeze())\n",
    "        val_pred = np.stack(val_pred, axis=0)\n",
    "        val_targ = np.stack(val_targ, axis=0)\n",
    "        for cell in range(n_units):\n",
    "            pearsons.append(pearsonr(val_pred[:,cell],val_targ[:,cell])[0])\n",
    "        model.train()\n",
    "        return np.array(pearsons).mean(), val_pred, val_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pc, pred, targ = pearsonr_eval(model, validation_data, 5, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4108218647685084"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40893975, 0.40948418, 0.41150495, 0.41919985, 0.42313787,\n",
       "       0.43426737, 0.47184646, 0.5339878 , 0.6174099 , 0.7082607 ,\n",
       "       0.79325217, 0.87827164, 0.9389497 , 0.9642763 , 0.9274244 ,\n",
       "       0.86866426, 0.833767  , 0.79222906, 0.7820203 , 0.8028346 ,\n",
       "       0.92672175, 1.2202032 , 1.5744325 , 1.79328   , 1.8598652 ,\n",
       "       1.7252023 , 1.3742914 , 0.8799336 , 0.5546501 , 0.51331186,\n",
       "       0.850829  , 1.8693075 , 3.5356889 , 5.5804615 , 7.7717314 ,\n",
       "       9.423938  , 9.440884  , 8.002787  , 6.2089586 , 4.394852  ,\n",
       "       3.0475838 , 2.0387793 , 1.3239295 , 0.92205596, 0.6628606 ,\n",
       "       0.46340144, 0.39761263, 0.35650983, 0.36034703, 0.41376388,\n",
       "       0.47783035, 0.5017842 , 0.46015722, 0.41990554, 0.4048472 ,\n",
       "       0.4015095 , 0.40133572, 0.41616258, 0.46790478, 0.564943  ,\n",
       "       0.6877945 , 0.7986342 , 0.8963872 , 0.9863641 , 1.0480021 ,\n",
       "       1.0761828 , 1.1376846 , 1.2883202 , 1.533076  , 1.8364303 ,\n",
       "       2.1991477 , 2.6149654 , 3.025171  , 3.3071122 , 3.3986673 ,\n",
       "       3.2093995 , 2.8641188 , 2.4070663 , 1.9156209 , 1.4286988 ,\n",
       "       1.0230386 , 0.69532716, 0.4763276 , 0.32717234, 0.24026677,\n",
       "       0.17050628, 0.134352  , 0.12093379, 0.1288317 , 0.16372566,\n",
       "       0.23452878, 0.36913213, 0.6367875 , 1.0932039 , 1.6416539 ,\n",
       "       2.0553265 , 2.2542617 , 2.191656  , 1.868125  , 1.4278538 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[21000:21100,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.48672163e-04, 1.34345990e-02,\n",
       "       4.45748442e-01, 5.43031516e+00, 2.42901565e+01, 3.98938240e+01,\n",
       "       2.40575334e+01, 5.32680261e+00, 4.33064104e-01, 1.29273006e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ[21000:21100,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(pred, targ, device):\n",
    "    batch_size = 512\n",
    "    loss_fn = nn.PoissonNLLLoss(log_input=False, reduction='sum').to(device)\n",
    "    loss = 0\n",
    "    n_batch = pred.shape[0] // batch_size\n",
    "    pred = torch.from_numpy(pred)\n",
    "    targ = torch.from_numpy(targ)\n",
    "    with torch.no_grad():\n",
    "        for idx in range(n_batch):\n",
    "            loss += loss_fn(pred[batch_size*idx:batch_size*(idx+1)].to(device), \n",
    "                            targ[batch_size*idx:batch_size*(idx+1)].to(device))\n",
    "    loss /= pred.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10.3883, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "eval_loss(pred, targ, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
