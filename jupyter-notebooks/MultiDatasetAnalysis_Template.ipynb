{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import h5py as h5\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../\")\n",
    "from models import PracticalBNCNN, NormedBNCNN, DalesBNCNN, DalesSSCNN, SSCNN, BNCNN, PracticalBNCNN, DalesHybrid, DalesSkipBNCNN, SkipBNBNCNN\n",
    "#import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.deepretina_loader import loadexpt\n",
    "from utils.physiology import Physio\n",
    "import utils.intracellular as intracellular\n",
    "import utils.batch_compute as bc\n",
    "import utils.retinal_phenomena as rp\n",
    "import utils.stimuli as stimuli\n",
    "import pyret.filtertools as ft\n",
    "import scipy\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import resource\n",
    "import time\n",
    "import math\n",
    "\n",
    "def normalize(x):\n",
    "    return (x-x.mean())/(x.std()+1e-7)\n",
    "\n",
    "def retinal_phenomena_figs(bn_cnn):\n",
    "    rp.step_response(bn_cnn)\n",
    "    rp.osr(bn_cnn)\n",
    "    rp.reversing_grating(bn_cnn)\n",
    "    rp.contrast_adaptation(bn_cnn, .35, .05)\n",
    "    rp.motion_anticipation(bn_cnn)\n",
    "    \n",
    "#If you want to use stimulus that isnt just boxes\n",
    "def prepare_stim(stim, stim_type):\n",
    "    if stim_type == 'boxes':\n",
    "        return 2*stim - 1\n",
    "    elif stim_type == 'flashes':\n",
    "        stim = stim.reshape(stim.shape[0], 1, 1)\n",
    "        return np.broadcast_to(stim, (stim.shape[0], 38, 38))\n",
    "    elif stim_type == 'movingbar':\n",
    "        stim = block_reduce(stim, (1,6), func=np.mean)\n",
    "        stim = pyret.stimulustools.upsample(stim.reshape(stim.shape[0], stim.shape[1], 1), 5)[0]\n",
    "        return np.broadcast_to(stim, (stim.shape[0], stim.shape[1], stim.shape[1]))\n",
    "    elif stim_type == 'lines':\n",
    "        stim_averaged = np.apply_along_axis(lambda m: np.convolve(m, 0.5*np.ones((2,)), mode='same'), \n",
    "                                            axis=1, arr=stim)\n",
    "        stim = stim_averaged[:,::2]\n",
    "        # now stack stimulus to convert 1d to 2d spatial stimulus\n",
    "        return stim.reshape(-1,1,stim.shape[-1]).repeat(stim.shape[-1], axis=1)\n",
    "    else:\n",
    "        print(\"Invalid stim type\")\n",
    "        assert False\n",
    "    \n",
    "def index_of(arg, arr):\n",
    "    for i in range(len(arr)):\n",
    "        if arg == arr[i]:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "# num_pots stores the number of cells per stimulus\n",
    "# mem_pots stores the membrane potential\n",
    "# psst, you can find the \"data\" folder in /home/grantsrb on deepretina server\n",
    "# psssst, note the additional ../ added to each path in files\n",
    "\n",
    "files = ['../data/bipolars_late_2012.h5', '../data/bipolars_early_2012.h5', '../data/amacrines_early_2012.h5', '../data/amacrines_late_2012.h5', '../data/horizontals_early_2012.h5', '../data/horizontals_late_2012.h5']\n",
    "files = [\"../\" + name for name in files]\n",
    "file_ids = []\n",
    "for f in files:\n",
    "    file_ids.append(re.split('_|\\.', f)[0])\n",
    "filter_length = 40\n",
    "window_size = 2\n",
    "num_pots = []\n",
    "stims = dict()\n",
    "mem_pots = dict()\n",
    "keys_to_use = {\"boxes\"}\n",
    "for fi in files:\n",
    "    with h5.File(fi, 'r') as f:\n",
    "        for k in f.keys():\n",
    "            if k in keys_to_use:\n",
    "                if k not in stims:\n",
    "                    stims[k] = []\n",
    "                if k not in mem_pots:\n",
    "                    mem_pots[k] = []\n",
    "                try:\n",
    "                    stims[k].append(prepare_stim(np.asarray(f[k+'/stimuli']), k))\n",
    "                    mem_pots[k].append(np.asarray(f[k]['detrended_membrane_potential'])[:, filter_length:])\n",
    "                except:\n",
    "                    print(\"stim error at\", k)\n",
    "        num = np.array(f['boxes/detrended_membrane_potential'].shape[0])\n",
    "        num_pots.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_folder = \"bncnnMultiData\"\n",
    "exp_folder = \"../training_scripts/\"+grand_folder\n",
    "_, model_folders, _ = next(os.walk(exp_folder))\n",
    "for i,f in enumerate(model_folders):\n",
    "    model_folders[i] = grand_folder + \"/\" + f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folders = sorted(model_folders, key=lambda x: (x.split(\"dataset\")[-1].split(\"_\")[0], x.split(\"stim_type\")[-1]))\n",
    "print(\"\\n\".join(model_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../training_scripts/\"+model_folders[0]+\"/test_epoch_0.pth\"\n",
    "try:\n",
    "    with open(file, \"rb\") as fd:\n",
    "        temp = torch.load(fd)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "temp['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = ['sequential.2', 'sequential.8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "print(\"Memory Used: {:.2f} mb\".format(max_mem_used / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "best_folder_by_loss = \"\"\n",
    "best_loss = 100\n",
    "best_folder_by_val_loss = \"\"\n",
    "best_val_loss = 100\n",
    "best_folder_by_val_acc = \"\"\n",
    "best_val_acc = -100\n",
    "best_folder_by_test_acc = \"\"\n",
    "best_test_acc = -100\n",
    "best_folder_by_intr_cor = \"\"\n",
    "best_intr_cor = -1\n",
    "\n",
    "results_file_name = grand_folder + \"_analysis_results.txt\"\n",
    "results_file = open(results_file_name, 'a')\n",
    "batch_compute_size = 500\n",
    "\n",
    "model_stats = dict()\n",
    "intraneuron_cors = dict()\n",
    "test_accs = dict()\n",
    "# load the losses\n",
    "print(\"Using layers:\", \" and \".join(conv_layers))\n",
    "for folder in model_folders:\n",
    "    stats = dict()\n",
    "    starttime = time.time()\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    for i in range(n_epochs):\n",
    "        file = \"../training_scripts/\"+folder+\"/test_epoch_{0}.pth\".format(i)\n",
    "        try:\n",
    "            with open(file, \"rb\") as fd:\n",
    "                temp = torch.load(fd)\n",
    "            losses.append(temp['loss'])\n",
    "            val_losses.append(temp['val_loss'])\n",
    "            val_accs.append(temp['val_acc'])\n",
    "        except:\n",
    "            break\n",
    "    bn_cnn = temp['model']\n",
    "    bn_cnn = bn_cnn.to(DEVICE)\n",
    "    bn_cnn.eval()\n",
    "    print(\"Folder:\", folder)\n",
    "    results_file.write(folder + \"\\n\")\n",
    "    print(\"Train Loss:\", losses[-1])\n",
    "    stats['TrainLoss'] = losses[-1]\n",
    "    results_file.write(\"Train Loss:\"+ str(losses[-1]) + \"\\n\")\n",
    "    print(\"Val Loss:\", val_losses[-1])\n",
    "    stats['ValLoss'] = val_losses[-1]\n",
    "    results_file.write(\"Val Loss:\"+ str(val_losses[-1]) + \"\\n\")\n",
    "    print(\"Val Acc:\", val_accs[-1])\n",
    "    stats['ValAcc'] = val_accs[-1]\n",
    "    results_file.write(\"Val Acc:\"+ str(val_accs[-1]) + \"\\n\")\n",
    "    if(math.isnan(losses[-1]) or math.isnan(val_losses[-1]) or math.isnan(val_accs[-1])):\n",
    "        print(\"NaN results, continuing...\\n\\n\\n\\n\")\n",
    "        results_file.write(\"NaN results, continuing...\\n\\n\\n\\n\")\n",
    "        continue\n",
    "    \n",
    "    cells = \"all\"\n",
    "    dataset = folder.split(\"dataset\")[-1].split(\"_\")[0]\n",
    "    stats['dataset'] = dataset\n",
    "    stim_type = folder.split(\"stim_type\")[-1].split(\"_\")[0]\n",
    "    stats['stim_type'] = stim_type\n",
    "    try:\n",
    "        norm_stats = [temp['norm_stats']['mean'], temp['norm_stats']['std']]\n",
    "    except:\n",
    "        norm_stats = [temp['data_norm_stats']['mean'], temp['data_norm_stats']['std']]\n",
    "    test_data = loadexpt(dataset,cells,stim_type,'test',40,0, norm_stats=norm_stats)\n",
    "    test_x = torch.from_numpy(test_data.X)\n",
    "    \n",
    "    model_response = bc.batch_compute_model_response(test_data.X, bn_cnn, batch_compute_size, insp_keys=set(conv_layers))    \n",
    "    avg_test_acc = np.mean([scipy.stats.pearsonr(model_response['output'][:, i], test_data.y[:, i])[0] for i in range(len(cells))])\n",
    "    test_accs[folder] = avg_test_acc\n",
    "    stats['TestAcc'] = avg_test_acc\n",
    "    if math.isnan(avg_test_acc):\n",
    "        print(\"NaN results, continuing...\\n\\n\\n\\n\")\n",
    "        results_file.write(\"NaN results, continuing...\\n\\n\\n\\n\")\n",
    "        continue\n",
    "    print(\"\\nFinal Test Acc:\", avg_test_acc)\n",
    "    results_file.write(\"Final Test Acc:\"+ str(avg_test_acc) + \"\\n\")\n",
    "    with open(\"../training_scripts/\"+folder+\"/hyperparams.txt\", 'a') as f:\n",
    "        f.write(\"\\nTest Ganglion Cell Correlation: \" + str(avg_test_acc))\n",
    "    \n",
    "    # Plot loss curve and response snippet\n",
    "    plt.plot(normalize(model_response['output'][:400, 0]))\n",
    "    plt.plot(normalize(test_data.y[:400,0]), alpha=.7)\n",
    "    plt.legend([\"model\", \"data\"])\n",
    "    plt.title(\"Firing Rate\")\n",
    "    plt.show()\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"TrainLoss\", \"ValLoss\"])\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.show()\n",
    "    retinal_phenomena_figs(bn_cnn)\n",
    "    plt.show()\n",
    "    \n",
    "    if avg_test_acc < .5 or losses[-1] > 1:\n",
    "        print(\"Skipping further analysis due to poor results...\\n\\n\\n\\n\")\n",
    "        results_file.write(\"Skipping further analysis due to poor results...\\n\\n\\n\\n\")\n",
    "        continue\n",
    "    print(\"Calculating model responses...\\n\")\n",
    "    # Computes the model responses for each stimulus \n",
    "    # and interneuron type labels y_true (0 for bipolar, 1 for amacrine, 2 for horizontal)\n",
    "    y_true = []\n",
    "    filter_length = 40\n",
    "    model_responses = dict()\n",
    "    cell_type_keys = ['bipolar', 'amacrine', 'horizontal']\n",
    "    for i in tqdm(range(len(files))):\n",
    "        file_name = files[i]\n",
    "        if cell_type_keys[0] in file_name:\n",
    "            for j in range(num_pots[i]):\n",
    "                y_true.append(0)\n",
    "        elif cell_type_keys[1] in file_name:\n",
    "            for j in range(num_pots[i]):\n",
    "                y_true.append(1)\n",
    "        elif cell_type_keys[2] in file_name:\n",
    "            for j in range(num_pots[i]):\n",
    "                y_true.append(2)\n",
    "        for k in stims.keys():\n",
    "            stim = stims[k][i]\n",
    "            padded_stim = intracellular.pad_to_edge(scipy.stats.zscore(stim))\n",
    "            if k not in model_responses:\n",
    "                model_responses[k] = []\n",
    "            model_responses[k].append(bc.batch_compute_model_response(stimuli.concat(padded_stim),\n",
    "                                                                      bn_cnn,batch_compute_size, \n",
    "                                                                      insp_keys=set(conv_layers)))\n",
    "            # Reshape potentially flat layers\n",
    "            for j,cl in enumerate(conv_layers):\n",
    "                if len(model_responses[k][-1][cl].shape) <= 2:\n",
    "                    try:\n",
    "                        model_responses[k][-1][cl] = model_responses[k][-1][cl].reshape((-1,8,36,36))\n",
    "                    except:\n",
    "                        model_responses[k][-1][cl] = model_responses[k][-1][cl].reshape((-1,8,26,26))\n",
    "    \n",
    "    # uses classify to get the most correlated cell/layer/subtype for each interneuron recording. \n",
    "    # Stored in all_cell_info. y_pred does a baseline \"classification\": record the convolutional \n",
    "    # layer that the most correlated cell is in.\n",
    "    # See intracellular.py for more info\n",
    "    # This takes a really long time to run. \n",
    "    print(\"Calculating intercellular correlations...\\n\")\n",
    "    all_cell_info = dict()\n",
    "    info_by_type = dict()\n",
    "    y_pred = dict()\n",
    "    for i in tqdm(range(len(files))):\n",
    "        interneuron = cell_type_keys[i//2]\n",
    "        for k in stims.keys():\n",
    "            model_response = model_responses[k][i]\n",
    "            stim = stims[k][i]\n",
    "            for j in range(mem_pots[k][i].shape[0]):\n",
    "                potential = mem_pots[k][i][j]\n",
    "                cell_info = intracellular.classify(potential, model_response, stim.shape[0], \n",
    "                                                   layer_keys=conv_layers)\n",
    "                #layer, channel,(row, col), cor_coef = cell_info\n",
    "                if k not in all_cell_info:\n",
    "                    info_by_type[k] = dict()\n",
    "                    all_cell_info[k] = []\n",
    "                    y_pred[k] = []\n",
    "                all_cell_info[k].append(cell_info)\n",
    "                y_pred[k].append(index_of(cell_info[0], conv_layers))\n",
    "    \n",
    "    stats['all_cell_info'] = all_cell_info\n",
    "    intraneuron_cors[folder] = all_cell_info\n",
    "            \n",
    "    # Average intracellular correlation. RIP.\n",
    "    avg_intr_cor = np.mean(np.asarray([[all_cell_info[k][i][-1] for i in range(len(all_cell_info[k]))] for k in all_cell_info.keys()]))\n",
    "    print(\"Mean intracellular:\", avg_intr_cor)\n",
    "    stats['IntrCor'] = avg_intr_cor\n",
    "    results_file.write(\"Mean intracellular:\" + str(avg_intr_cor) + \"\\n\")\n",
    "    std = np.std(np.asarray([[all_cell_info[k][i][-1] for i in range(len(all_cell_info[k]))] for k in all_cell_info.keys()]))\n",
    "    print(\"Std intracellular:\", std)\n",
    "    m = np.min(np.asarray([[all_cell_info[k][i][-1] for i in range(len(all_cell_info[k]))] for k in all_cell_info.keys()]))\n",
    "    print(\"Min intracellular:\", m)\n",
    "    results_file.write(\"Min intracellular:\" + str(m) + \"\\n\")\n",
    "    m = np.max(np.asarray([[all_cell_info[k][i][-1] for i in range(len(all_cell_info[k]))] for k in all_cell_info.keys()]))\n",
    "    print(\"Max intracellular:\", m)\n",
    "    results_file.write(\"Max intracellular:\" + str(m) + \"\\n\")\n",
    "    \n",
    "    if avg_intr_cor > best_intr_cor:\n",
    "        best_intr_cor = avg_intr_cor\n",
    "        best_folder_by_intr_cor = folder\n",
    "    \n",
    "    stim_type = 'boxes'\n",
    "    # Make example correlation map\n",
    "    model_response = model_responses[stim_type][-1]\n",
    "    potential = mem_pots[stim_type][-1][-1]\n",
    "    layer, k, (i,j), r = all_cell_info[stim_type][-1]\n",
    "    print(\"Layer\", layer, \"correlation map\")\n",
    "    plt.imshow(intracellular.correlation_map(potential, model_response[layer][:, k]))\n",
    "    plt.show()\n",
    "\n",
    "    layer_dict = {} # Tracks the number of cells that were classified to each layer in conv_layers\n",
    "    # Tally layers for maximally correlated cell\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] not in layer_dict:\n",
    "            layer_dict[y_true[i]] = [0 for i in range(len(conv_layers))]\n",
    "        for k in y_pred.keys():\n",
    "            layer_dict[y_true[i]][y_pred[k][i]] += 1\n",
    "\n",
    "    width = 0.5\n",
    "    lkeys = list(layer_dict.keys())\n",
    "    ind = np.arange(0,len(conv_layers))\n",
    "    for i,k in enumerate(lkeys):\n",
    "        plt.bar(ind, [count for count in layer_dict[k]], width)\n",
    "        plt.xticks(ind,conv_layers)\n",
    "        print(cell_type_keys[i])\n",
    "        plt.title(\"Layer of unit with max correlation\")\n",
    "        plt.show()\n",
    "    \n",
    "    stimulus_num = 3\n",
    "    filter_length = 40\n",
    "    for type_key in stims.keys():\n",
    "        if type_key == \"flashes\":\n",
    "            continue\n",
    "        stimulus = stims[type_key][stimulus_num]\n",
    "        # Plot the receptive field for a model cell\n",
    "        for i,cl in enumerate(conv_layers):\n",
    "            model_cell_response = model_responses[type_key][stimulus_num][cl][:, 1, 15, 15]\n",
    "            print(\"Receptive field of\", type_key,\"model cell in Layer\", i)\n",
    "            rc_model, lags_model = ft.revcorr(scipy.stats.zscore(stimulus)[filter_length:], model_cell_response, \n",
    "                                              nsamples_before=0, nsamples_after=filter_length)\n",
    "            spatial_model, temporal_model = ft.decompose(rc_model)\n",
    "            img = plt.imshow(spatial_model, cmap = 'seismic', clim=[-np.max(abs(spatial_model)), \n",
    "                                                                   np.max(abs(spatial_model))])\n",
    "            plt.show()\n",
    "    \n",
    "    gc.collect()\n",
    "    max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    print(\"Memory Used: {:.2f} memory\".format(max_mem_used / 1024))\n",
    "    print(\"Completed in\", time.time()-starttime, \"seconds\")\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    results_file.write(\"\\n\\n\\n\\n\")\n",
    "    model_stats[folder] = stats\n",
    "                    \n",
    "print(\"Best by validation loss:\", best_folder_by_val_loss)\n",
    "print(\"Best by training loss:\", best_folder_by_loss)\n",
    "print(\"Best by val accuracy:\", best_folder_by_val_acc)\n",
    "print(\"Best by test accuracy:\", best_folder_by_test_acc)\n",
    "print(\"Best by intracellular correlation:\", best_folder_by_intr_cor)\n",
    "results_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(model_stats, metric):\n",
    "    best_models = dict()\n",
    "    for folder in model_stats.keys():\n",
    "        stats = model_stats[folder]\n",
    "        if stats['dataset'] not in best_models:\n",
    "            best_models[stats['dataset']] = dict()\n",
    "        if stats['stim_type'] not in best_models[stats['dataset']]:\n",
    "            best_models[stats['dataset']][stats['stim_type']] = folder\n",
    "        best_folder = best_models[stats['dataset']][stats['stim_type']]\n",
    "        if model_stats[best_folder][metric] < stats[metric]:\n",
    "            best_models[stats['dataset']][stats['stim_type']] = folder\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on models best by test acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_folders = get_best_models(model_stats, \"TestAcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = []\n",
    "for k in best_folders.keys():\n",
    "    folders += [best_folders[k][kk] for kk in best_folders[k].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg Test Acc:\", np.mean([model_stats[folder]['TestAcc'] for folder in folders]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(best_folders.keys())\n",
    "print(\"Nat Scenes:\", np.mean([model_stats[best_folders[key]['naturalscene']]['TestAcc'] for key in keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(best_folders.keys())\n",
    "print(\"Whitenoise:\", np.mean([model_stats[best_folders[key]['whitenoise']]['TestAcc'] for key in keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cors = {k:[] for k in cell_type_keys} # cell_type_keys are defined above as ['bipolar', 'amacrine', 'horizontal']\n",
    "for dataset in best_folders.keys():\n",
    "    for stim_type in best_folders[dataset].keys():\n",
    "        all_cell_info = model_stats[best_folders[dataset][stim_type]]['all_cell_info']\n",
    "        for intr_stim_type in all_cell_info.keys(): # each stim type for the intraneuron recordings\n",
    "            for j in range(len(all_cell_info[intr_stim_type])):\n",
    "                max_cor = all_cell_info[intr_stim_type][j][-1]\n",
    "                avg_cors[cell_type_keys[y_true[j]]].append(max_cor)\n",
    "for k in avg_cors:\n",
    "    print(\"Mean\", k, \":\", np.mean(avg_cors[k]))\n",
    "grand_avg = []\n",
    "for k in avg_cors:\n",
    "    grand_avg += [x for x in avg_cors[k]]\n",
    "print(\"Mean across all:\", np.mean(grand_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
