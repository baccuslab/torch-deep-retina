{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import h5py as h5\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from torchdeepretina.models import *\n",
    "#import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdeepretina.deepretina_loader import loadexpt\n",
    "from torchdeepretina.physiology import Physio\n",
    "import torchdeepretina.intracellular as intracellular\n",
    "import torchdeepretina.batch_compute as bc\n",
    "import torchdeepretina.retinal_phenomena as rp\n",
    "import torchdeepretina.stimuli as stimuli\n",
    "import pyret.filtertools as ft\n",
    "import scipy\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import resource\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def normalize(x):\n",
    "    return (x-x.mean())/(x.std()+1e-7)\n",
    "\n",
    "def retinal_phenomena_figs(bn_cnn):\n",
    "    print(\"Step Response\")\n",
    "    rp.step_response(bn_cnn)\n",
    "    plt.show()\n",
    "    print(\"OSR\")\n",
    "    rp.osr(bn_cnn, duration=1)\n",
    "    plt.show()\n",
    "    print(\"Reversing Grating\")\n",
    "    rp.reversing_grating(bn_cnn)\n",
    "    plt.show()\n",
    "    print(\"Contrast Adaptation\")\n",
    "    rp.contrast_adaptation(bn_cnn, .35, .05)\n",
    "    plt.show()\n",
    "    contrasts = [0.5, 1.2]\n",
    "#     for i in range(bn_cnn.sequential[-3].weight.shape[0]):\n",
    "#         rp.contrast_fig(bn_cnn, contrasts, unit_index=i, nonlinearity_type=\"sigmoid\")\n",
    "    unit = 0\n",
    "    print(\"Contrast fig for unit:\", unit)\n",
    "    rp.contrast_fig(bn_cnn, contrasts, unit_index=unit, nonlinearity_type=\"sigmoid\")\n",
    "    plt.show()\n",
    "    print(\"Motion Reversal\")\n",
    "    rp.motion_reversal(bn_cnn)\n",
    "    plt.show()\n",
    "    print(\"Motion Anticipation\")\n",
    "    rp.motion_anticipation(bn_cnn)\n",
    "    plt.show()\n",
    "    print(\"OMS\")\n",
    "    fig, diff_vid, global_vid, diff_response, global_response = rp.oms_differential(bn_cnn)\n",
    "    plt.show()\n",
    "    print(\"Ratios:\", global_response.mean(0)/diff_response.mean(0))\n",
    "    \n",
    "#If you want to use stimulus that isnt just boxes\n",
    "def prepare_stim(stim, stim_type):\n",
    "    if stim_type == 'boxes':\n",
    "        return 2*stim - 1\n",
    "    elif stim_type == 'flashes':\n",
    "        stim = stim.reshape(stim.shape[0], 1, 1)\n",
    "        return np.broadcast_to(stim, (stim.shape[0], 38, 38))\n",
    "    elif stim_type == 'movingbar':\n",
    "        stim = block_reduce(stim, (1,6), func=np.mean)\n",
    "        stim = pyret.stimulustools.upsample(stim.reshape(stim.shape[0], stim.shape[1], 1), 5)[0]\n",
    "        return np.broadcast_to(stim, (stim.shape[0], stim.shape[1], stim.shape[1]))\n",
    "    elif stim_type == 'lines':\n",
    "        stim_averaged = np.apply_along_axis(lambda m: np.convolve(m, 0.5*np.ones((2,)), mode='same'), \n",
    "                                            axis=1, arr=stim)\n",
    "        stim = stim_averaged[:,::2]\n",
    "        # now stack stimulus to convert 1d to 2d spatial stimulus\n",
    "        return stim.reshape(-1,1,stim.shape[-1]).repeat(stim.shape[-1], axis=1)\n",
    "    else:\n",
    "        print(\"Invalid stim type\")\n",
    "        assert False\n",
    "    \n",
    "def index_of(arg, arr):\n",
    "    for i in range(len(arr)):\n",
    "        if arg == arr[i]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def make_data_frame(model_stats, headers):\n",
    "    data = dict()\n",
    "    for header in headers:\n",
    "        data[header] = []\n",
    "    for folder in model_stats.keys():\n",
    "        untouched_keys = set(data.keys())\n",
    "        for k in model_stats[folder].keys():\n",
    "            if k in data:\n",
    "                data[k].append(model_stats[folder][k])\n",
    "                if k in untouched_keys:\n",
    "                    untouched_keys.remove(k)\n",
    "        with open(\"../training_scripts/\" + folder+\"/hyperparams.txt\") as f:\n",
    "            architecture = []\n",
    "            for i,line in enumerate(f):\n",
    "                if \"(\" in line or \")\" in line:\n",
    "                    l = line.replace(\"\\n\", \"#\")\n",
    "                    architecture.append(l)\n",
    "                else:\n",
    "                    splt_line = line.strip().split(\":\")\n",
    "                    if len(splt_line) == 2 and splt_line[0].strip() in data:\n",
    "                        data[splt_line[0].strip()].append(splt_line[1].strip())\n",
    "                        if splt_line[0].strip() in untouched_keys:\n",
    "                            untouched_keys.remove(splt_line[0].strip())\n",
    "            data['architecture'].append(\"\".join(architecture))\n",
    "            untouched_keys.remove('architecture')\n",
    "        for k in list(untouched_keys):\n",
    "            data[k].append(None)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_model(folder, data):\n",
    "    try:\n",
    "        hyps=get_hyps(folder)\n",
    "        hyps['model_type'] = hyps['model_type'].split(\".\")[-1].split(\"\\'\")[0].strip()\n",
    "        hyps['model_type'] = globals()[hyps['model_type']]\n",
    "        bn_cnn = hyps['model_type'](**data['model_hyps'])\n",
    "    except Exception as e:\n",
    "        model_hyps = {\"n_units\":5,\"noise\":float(hyps['noise'])}\n",
    "        if \"bias\" in hyps:\n",
    "            model_hyps['bias'] = hyps['bias'] == \"True\"\n",
    "        if \"chans\" in hyps:\n",
    "            model_hyps['chans'] = [int(x) for x in \n",
    "                                   hyps['chans'].replace(\"[\", \"\").replace(\"]\", \"\").strip().split(\",\")]\n",
    "        if \"adapt_gauss\" in hyps:\n",
    "            model_hyps['adapt_gauss'] = hyps['adapt_gauss'] == \"True\"\n",
    "        if \"linear_bias\" in hyps:\n",
    "            model_hyps['linear_bias'] = hyps['linear_bias'] == \"True\"\n",
    "        if \"softplus\" in hyps:\n",
    "            model_hyps['softplus'] = hyps['softplus'] == \"True\"\n",
    "        fn_args = set(hyps['model_type'].__init__.__code__.co_varnames)\n",
    "        for k in model_hyps.keys():\n",
    "            if k not in fn_args:\n",
    "                del model_hyps[k]\n",
    "        bn_cnn = hyps['model_type'](**model_hyps)\n",
    "    return bn_cnn\n",
    "\n",
    "def get_hyps(folder):\n",
    "    hyps = dict()\n",
    "    with open(os.path.join(\"../training_scripts/\", folder, \"hyperparams.txt\")) as f:\n",
    "        for line in f:\n",
    "            if \"(\" not in line and \")\" not in line:\n",
    "                splt = line.strip().split(\":\")\n",
    "                if len(splt) > 1:\n",
    "                    hyps[splt[0]] = splt[1].strip()\n",
    "    return hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "# num_pots stores the number of cells per stimulus\n",
    "# mem_pots stores the membrane potential\n",
    "# psst, you can find the \"data\" folder in /home/grantsrb on deepretina server\n",
    "# psssst, note the additional ../ added to each path in files\n",
    "root_path = \"~/interneuron_data/\"\n",
    "files = ['bipolars_late_2012.h5', 'bipolars_early_2012.h5', 'amacrines_early_2012.h5', \n",
    "         'amacrines_late_2012.h5', 'horizontals_early_2012.h5', 'horizontals_late_2012.h5']\n",
    "files = [os.path.expanduser(root_path + name) for name in files]\n",
    "file_ids = []\n",
    "for f in files:\n",
    "    file_ids.append(re.split('_|\\.', f)[0])\n",
    "filter_length = 40\n",
    "window_size = 2\n",
    "num_pots = []\n",
    "stims = dict()\n",
    "mem_pots = dict()\n",
    "keys_to_use = {\"boxes\"}\n",
    "for fi in files:\n",
    "    with h5.File(fi, 'r') as f:\n",
    "        for k in f.keys():\n",
    "            if k in keys_to_use:\n",
    "                if k not in stims:\n",
    "                    stims[k] = []\n",
    "                if k not in mem_pots:\n",
    "                    mem_pots[k] = []\n",
    "                try:\n",
    "                    stims[k].append(prepare_stim(np.asarray(f[k+'/stimuli']), k))\n",
    "                    mem_pots[k].append(np.asarray(f[k]['detrended_membrane_potential'])[:, filter_length:])\n",
    "                except:\n",
    "                    print(\"stim error at\", k)\n",
    "        num = np.array(f['boxes/detrended_membrane_potential'].shape[0])\n",
    "        num_pots.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_folder = \"absbnbncnn\"\n",
    "exp_folder = \"../training_scripts/\"+grand_folder\n",
    "_, model_folders, _ = next(os.walk(exp_folder))\n",
    "for i,f in enumerate(model_folders):\n",
    "    model_folders[i] = grand_folder + \"/\" + f\n",
    "model_folders = sorted(model_folders)\n",
    "print(\"\\n\".join(model_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../training_scripts/\"+model_folders[0]+\"/test_epoch_0.pth\"\n",
    "try:\n",
    "    with open(file, \"rb\") as fd:\n",
    "        temp = torch.load(fd)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    temp['model']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    bn_cnn = load_model(model_folders[0], temp)\n",
    "print(bn_cnn)\n",
    "try:\n",
    "    chans = temp['model_hyps']['chans']\n",
    "    print(\"Chans:\", chans)\n",
    "except:\n",
    "    print(\"Chans not recorded in model_hyps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = ['sequential.2', 'sequential.8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = \"all\"\n",
    "dataset = '15-10-07'\n",
    "stim_type = 'naturalscene'\n",
    "try:\n",
    "    norm_stats = [temp['norm_stats']['mean'], temp['norm_stats']['std']]\n",
    "except:\n",
    "    norm_stats = [51.49175, 53.62663279042969]\n",
    "test_data = loadexpt(dataset,cells,stim_type,'test',40,0, norm_stats=norm_stats)\n",
    "test_x = torch.from_numpy(test_data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "print(\"Memory Used: {:.2f} mb\".format(max_mem_used / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "batch_compute_size = 500\n",
    "model_stats = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Using layers:\", \" and \".join(conv_layers))\n",
    "for folder in model_folders:\n",
    "    model_stats[folder] = dict()\n",
    "    starttime = time.time()\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    for i in range(n_epochs):\n",
    "        file = \"../training_scripts/\"+folder+\"/test_epoch_{0}.pth\".format(i)\n",
    "        try:\n",
    "            with open(file, \"rb\") as fd:\n",
    "                temp = torch.load(fd)\n",
    "            losses.append(temp['loss'])\n",
    "            val_losses.append(temp['val_loss'])\n",
    "            val_accs.append(temp['val_acc'])\n",
    "            data = temp\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        bn_cnn = data['model']\n",
    "    except Exception as e:\n",
    "        bn_cnn = load_model(folder, data)\n",
    "    try:\n",
    "        bn_cnn.load_state_dict(data['model_state_dict'])\n",
    "    except RuntimeError as e:\n",
    "        data['model_state_dict'][\"sequential.3.sigma\"] = data['model_state_dict'][\"sequential.3.cuda_param\"]\n",
    "        del data['model_state_dict'][\"sequential.3.cuda_param\"]\n",
    "        data['model_state_dict'][\"sequential.10.sigma\"] = data['model_state_dict'][\"sequential.10.cuda_param\"]\n",
    "        del data['model_state_dict'][\"sequential.10.cuda_param\"]\n",
    "        bn_cnn.load_state_dict(data['model_state_dict'])\n",
    "    try:\n",
    "        chans = temp['model_hyps']['chans']\n",
    "    except:\n",
    "        chans=[8,8]\n",
    "    bn_cnn = bn_cnn.to(DEVICE)\n",
    "    bn_cnn.eval()\n",
    "    print(\"Folder:\", folder)\n",
    "    print(\"Final Loss:\", losses[-1])\n",
    "    model_stats[folder]['FinalLoss'] = losses[-1]\n",
    "    print(\"Final Val:\", val_losses[-1])\n",
    "    model_stats[folder]['FinalVal'] = val_losses[-1]\n",
    "    print(\"Final Val Acc:\", val_accs[-1])\n",
    "    model_stats[folder]['ValAcc'] = val_accs[-1]\n",
    "    if(math.isnan(losses[-1]) or math.isnan(val_losses[-1]) or math.isnan(val_accs[-1])):\n",
    "        print(\"NaN results, continuing...\\n\\n\\n\\n\")\n",
    "        continue\n",
    "\n",
    "    model_response = bc.batch_compute_model_response(test_data.X, bn_cnn, batch_compute_size, \n",
    "                                                     insp_keys=set(conv_layers))\n",
    "    test_accs = [scipy.stats.pearsonr(model_response['output'][:, i], test_data.y[:, i])[0] \n",
    "                                                    for i in range(test_data.y.shape[-1])]\n",
    "    avg_test_acc = np.mean(test_accs)\n",
    "    for i, cell in enumerate(test_data.cells):\n",
    "        model_stats[folder][\"cell\"+str(cell)] = test_accs[i]\n",
    "    if math.isnan(avg_test_acc):\n",
    "        print(\"NaN results, continuing...\\n\\n\\n\\n\")\n",
    "        continue\n",
    "    print(\"\\nFinal Test Acc:\", avg_test_acc)\n",
    "    model_stats[folder]['FinalTestAcc'] = avg_test_acc\n",
    "    with open(\"../training_scripts/\"+folder+\"/hyperparams.txt\", 'a') as f:\n",
    "        f.write(\"\\nTest Ganglion Cell Correlation: \" + str(avg_test_acc))\n",
    "    plt.plot(normalize(model_response['output'][:400, 0]))\n",
    "    plt.plot(normalize(test_data.y[:400,0]), alpha=.7)\n",
    "    plt.legend([\"model\", \"data\"])\n",
    "    plt.title(\"Firing Rate\")\n",
    "    plt.show()\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"TrainLoss\", \"ValLoss\"])\n",
    "    plt.title(\"Loss Curves\")\n",
    "#     plt.ylim([-6, 6])\n",
    "    plt.show()\n",
    "    retinal_phenomena_figs(bn_cnn)\n",
    "    plt.show()\n",
    "    \n",
    "    if avg_test_acc < .4 or losses[-1] > 1:\n",
    "        print(\"Skipping further analysis due to poor results...\\n\\n\\n\\n\")\n",
    "        continue\n",
    "    print(\"Calculating model responses...\\n\")\n",
    "    # Computes the model responses for each stimulus \n",
    "    # and interneuron type labels y_true (0 for bipolar, 1 for amacrine, 2 for horizontal)\n",
    "    y_true = []\n",
    "    filter_length = 40\n",
    "    model_responses = dict()\n",
    "    for i in tqdm(range(len(files))):\n",
    "        file_name = files[i]\n",
    "        if 'bipolar' in file_name:\n",
    "            for j in range(num_pots[i]):\n",
    "                y_true.append(0)\n",
    "        elif 'amacrine' in file_name:\n",
    "            for j in range(num_pots[i]):\n",
    "                y_true.append(1)\n",
    "        else:\n",
    "            for j in range(num_pots[i]):\n",
    "                y_true.append(2)\n",
    "        for k in stims.keys():\n",
    "            stim = stims[k][i]\n",
    "            padded_stim = intracellular.pad_to_edge(scipy.stats.zscore(stim))\n",
    "            if k not in model_responses:\n",
    "                model_responses[k] = []\n",
    "            model_responses[k].append(bc.batch_compute_model_response(stimuli.concat(padded_stim),\n",
    "                                                                      bn_cnn,batch_compute_size, \n",
    "                                                                      insp_keys=set(conv_layers)))\n",
    "            # Reshape potentially flat layers\n",
    "            for j,cl in enumerate(conv_layers):\n",
    "                if len(model_responses[k][-1][cl].shape) <= 2:\n",
    "                    try:\n",
    "                        model_responses[k][-1][cl] = model_responses[k][-1][cl].reshape((-1,chans[0],36,36))\n",
    "                    except:\n",
    "                        model_responses[k][-1][cl] = model_responses[k][-1][cl].reshape((-1,chans[1],26,26))\n",
    "    \n",
    "    # uses classify to get the most correlated cell/layer/subtype for each interneuron recording. \n",
    "    # Stored in all_cell_info. y_pred does a baseline \"classification\": record the convolutional \n",
    "    # layer that the most correlated cell is in.\n",
    "    # See intracellular.py for more info\n",
    "    # y_pred holds the layer that was maximally correlated between the conv_layers \n",
    "    print(\"Calculating intracellular correlations...\\n\")\n",
    "    all_cell_info = dict()\n",
    "    y_pred = dict()\n",
    "    for i in tqdm(range(len(files))):\n",
    "        for k in stims.keys():\n",
    "            model_response = model_responses[k][i]\n",
    "            stim = stims[k][i]\n",
    "            for j in range(mem_pots[k][i].shape[0]):\n",
    "                potential = mem_pots[k][i][j]\n",
    "                cell_info = intracellular.classify(potential, model_response, stim.shape[0], \n",
    "                                                   layer_keys=conv_layers)\n",
    "                #layer, channel,(row, col), cor_coef = cell_info\n",
    "                if k not in all_cell_info:\n",
    "                    all_cell_info[k] = []\n",
    "                    y_pred[k] = []\n",
    "                all_cell_info[k].append(cell_info)\n",
    "                y_pred[k].append(index_of(cell_info[0], conv_layers))\n",
    "    \n",
    "    model_stats[folder]['all_cell_info'] = all_cell_info\n",
    "    \n",
    "    # Collect intracellular correlations \n",
    "    cell_types = [\"bipolar\", \"amacrine\", \"horizontal\"]\n",
    "    idx_to_cell = {i:t for i,t in enumerate(cell_types)}\n",
    "    cors = {\"bipolar\":[], \"amacrine\":[], \"horizontal\":[]}\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        cell_type = idx_to_cell[idx]\n",
    "        # Add correlations from all stimulus types for this particular interneuron\n",
    "        cors[cell_type].extend([all_cell_info[k][i][-1] for k in all_cell_info.keys()])\n",
    "    # Collect average correlation coefficient for each cell type\n",
    "    for key in cors.keys():\n",
    "        model_stats[folder][key+\"_intr_cor\"] = np.mean(cors[key])\n",
    "        print(key, \"intracellular:\", model_stats[folder][key+\"_intr_cor\"])\n",
    "        \n",
    "    # Average intracellular correlation. RIP.\n",
    "    avg_intr_cor = np.mean(np.asarray([[all_cell_info[k][i][-1] for i in range(len(all_cell_info[k]))] for k in all_cell_info.keys()]))\n",
    "    print(\"Mean intracellular:\", avg_intr_cor)\n",
    "    model_stats[folder]['Meanintracellular'] = avg_intr_cor\n",
    "    std = np.std(np.asarray([[all_cell_info[k][i][-1] for i in range(len(all_cell_info[k]))] for k in all_cell_info.keys()]))\n",
    "    print(\"Std intracellular:\", std)\n",
    "    m = np.min(np.asarray([[all_cell_info[k][i][-1] for i in range(len(all_cell_info[k]))] for k in all_cell_info.keys()]))\n",
    "    print(\"Min intracellular:\", m)\n",
    "    model_stats[folder][\"Minintracellular\"] = m\n",
    "    m = np.max(np.asarray([[all_cell_info[k][i][-1] for i in range(len(all_cell_info[k]))] for k in all_cell_info.keys()]))\n",
    "    print(\"Max intracellular:\", m)\n",
    "    model_stats[folder][\"Maxintracellular\"] = m\n",
    "    \n",
    "    stim_type = 'boxes'\n",
    "    # Make example correlation map\n",
    "    model_response = model_responses[stim_type][-1]\n",
    "    potential = mem_pots[stim_type][-1][-1]\n",
    "    layer, k, (i,j), r = all_cell_info[stim_type][-1]\n",
    "    print(\"Layer\", layer, \"correlation map\")\n",
    "    plt.imshow(intracellular.correlation_map(potential, model_response[layer][:, k]))\n",
    "    plt.show()\n",
    "\n",
    "    keys = cell_types\n",
    "    layer_dict = {}\n",
    "    # Tally layers for maximally correlated cell\n",
    "    # y_true holds the true neuron type for each interneuron (0 is bipolar, 1 is amacrine, 2 is horizontal)\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] not in layer_dict:\n",
    "            layer_dict[y_true[i]] = [0 for i in range(len(conv_layers))]\n",
    "        for k in y_pred.keys():\n",
    "            layer_dict[y_true[i]][y_pred[k][i]] += 1 # tallys for each neuron type the layer of max correlation\n",
    "\n",
    "    width = 0.5\n",
    "    lkeys = list(layer_dict.keys())\n",
    "    ind = np.arange(0,len(conv_layers))\n",
    "    for i,k in enumerate(lkeys):\n",
    "        plt.bar(ind, [count for count in layer_dict[k]], width)\n",
    "        plt.xticks(ind,conv_layers)\n",
    "        print(keys[i])\n",
    "        plt.title(\"Layer of unit with max correlation\")\n",
    "        plt.show()\n",
    "    \n",
    "#     layer_dict = {keys[k]:v for k,v in layer_dict.items()}\n",
    "#     # layer dict now is {\"bipolar\":[layer1count, layer2count], \"amacrine\":...}\n",
    "#     model_stats[folder]['max_cor_layer_tallys'] = layer_dict\n",
    "    \n",
    "    stimulus_num = 3\n",
    "    filter_length = 40\n",
    "    for type_key in stims.keys():\n",
    "        if type_key == \"flashes\":\n",
    "            continue\n",
    "        stimulus = stims[type_key][stimulus_num]\n",
    "        # Plot the receptive field for a model cell\n",
    "        for i,cl in enumerate(conv_layers):\n",
    "            model_cell_response = model_responses[type_key][stimulus_num][cl][:, 1, 15, 15]\n",
    "            print(\"Receptive field of\", type_key,\"model cell in Layer\", i)\n",
    "            rc_model, lags_model = ft.revcorr(scipy.stats.zscore(stimulus)[filter_length:], model_cell_response, \n",
    "                                              nsamples_before=0, nsamples_after=filter_length)\n",
    "            spatial_model, temporal_model = ft.decompose(rc_model)\n",
    "            img = plt.imshow(spatial_model, cmap = 'seismic', clim=[-np.max(abs(spatial_model)), \n",
    "                                                                   np.max(abs(spatial_model))])\n",
    "            plt.show()\n",
    "    \n",
    "    gc.collect()\n",
    "    max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    print(\"Memory Used: {:.2f} memory\".format(max_mem_used / 1024))\n",
    "    print(\"Completed in\", time.time()-starttime, \"seconds\")\n",
    "    print(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_csv = 'models.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models.csv') as f:\n",
    "    headers = f.readline().strip().split(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add headers\n",
    "# head_set = set(headers)\n",
    "# missing_keys = set()\n",
    "# for folder in model_stats.keys():\n",
    "#     model_hyps = get_hyps(folder)\n",
    "#     for k in model_hyps.keys():\n",
    "#         if k not in head_set:\n",
    "#             missing_keys.add(k)\n",
    "# missing_keys\n",
    "# headers.append('bnorm_momentum')\n",
    "# headers.append('linear_bias')\n",
    "\n",
    "# main_frame = pd.read_csv(models_csv, delimiter=\"!\")\n",
    "# main_frame = main_frame.reindex(headers, axis=1)\n",
    "# main_frame.to_csv(models_csv, header=True, mode='w', sep=\"!\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdframe = make_data_frame(model_stats, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"exp_num\", \"lr\", \"l1\", \"l2\", \"noise\", \"adapt_gauss\", \"bias\", \"linear_bias\", \"chans\", \"lossfxn\", \"FinalTestAcc\", \"Meanintracellular\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pdframe.loc[:, cols]\n",
    "frame.sort_values(by=\"FinalTestAcc\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to comprehensive model csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdframe = pdframe.reindex(headers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdframe.to_csv(models_csv, header=False, mode='a', sep=\"!\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
