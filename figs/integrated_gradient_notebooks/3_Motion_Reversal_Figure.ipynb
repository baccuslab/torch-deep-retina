{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3260127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2_response is unavailable until you run:\n",
      "$ pip install -e git+git://github.com/nirum/jetpack.git@master#egg=jetpack\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "torch.cuda.device_count()  # print 1\n",
    "import torchdeepretina as tdr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchdeepretina.io as tdrio\n",
    "import torchdeepretina.utils as tdrutils\n",
    "import stimuli as s\n",
    "import torchdeepretina.stimuli as tdrstim\n",
    "from tqdm import tqdm\n",
    "from itertools import repeat\n",
    "from matplotlib import ticker, cm\n",
    "from colormap import Colormap\n",
    "c = Colormap()\n",
    "mycmap = c.cmap_linear('#2378FF','#FFFFFF','#FF3C3C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c475fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# load model given path\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    in: model_path\n",
    "    out: model\n",
    "    \"\"\"\n",
    "    model = tdrio.load_model(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_dir_path = '/home/htanaka/torch-deep-retina/models_paper/'#/tanaka/convgc_models/gcchansearch_57_chans[4, 4]/'\n",
    "\n",
    "#model_n_path = 'convgc_15-10-07_whitenoise'\n",
    "\n",
    "#model_n_path = '15-11-21b_naturalscene'\n",
    "#model_n_path = '15-10-07_naturalscene'\n",
    "#model_n_path = 'convgc_15-10-07_naturalscene'\n",
    "\n",
    "model_n_path = 'convgc_15-11-21b_naturalscene'\n",
    "model_n = load_model(model_dir_path + model_n_path + '.pt')\n",
    "model_n = tdr.utils.stacked2conv(model_n)\n",
    "\n",
    "model_w_path = 'convgc_15-11-21b_whitenoise'\n",
    "model_w = load_model(model_dir_path + model_w_path + '.pt')\n",
    "model_w = tdr.utils.stacked2conv(model_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ade4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion anticipation\n",
    "def motion_anticipation(velocity=0.08, width=6, flash_duration=2, filt_depth=40):\n",
    "    c_right, speed_right, stim_right = tdrstim.driftingbar(velocity,    \n",
    "                                                 width, x=(-30, 30))    \n",
    "    rightward_moving_bar = torch.from_numpy(stim_right)\n",
    "    c_left, speed_left, stim_left = tdrstim.driftingbar(-velocity,      \n",
    "                                               width, x=(30, -30))      \n",
    "    leftward_moving_bar = torch.from_numpy(stim_left)\n",
    "    flash_centers = np.arange(-25, 26)\n",
    "    flashes = (tdrstim.flash(flash_duration, 43, 70,                    \n",
    "                        intensity=tdrstim.bar((x, 0), width, 50)) for x in flash_centers)\n",
    "    xs = []\n",
    "    for f in flashes:\n",
    "        x = torch.from_numpy(tdrstim.concat(f, nh=filt_depth))\n",
    "        xs.append(x)\n",
    "    return xs, rightward_moving_bar, c_right, leftward_moving_bar, c_left\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Motion reversal\n",
    "def run_motion_reversal(x_locations=np.arange(-9, 3), speed=0.19, clip_n=210, scaling=1):\n",
    "    \"\"\"Gets responses to a bar reversing motion.\"\"\"                        \n",
    "    tflips, Xs = zip(*[s.motion_reversal(xi,speed)[1:] for xi in x_locations])\n",
    "    return tflips, Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f83155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion anticipation\n",
    "def motion_anticipation(velocity=0.08, width=6, flash_duration=2, filt_depth=40):\n",
    "    # Generate rightward moving bar stimulus\n",
    "    c_right, speed_right, stim_right = tdrstim.driftingbar(velocity, width, x=(-30, 30))\n",
    "    rightward_moving_bar = torch.from_numpy(stim_right)\n",
    "    \n",
    "    # Generate leftward moving bar stimulus\n",
    "    c_left, speed_left, stim_left = tdrstim.driftingbar(-velocity, width, x=(30, -30))\n",
    "    leftward_moving_bar = torch.from_numpy(stim_left)\n",
    "    \n",
    "    # Generate flashes at different positions\n",
    "    flash_centers = np.arange(-25, 26)\n",
    "    flashes = [tdrstim.flash(flash_duration, 43, 70, intensity=tdrstim.bar((x, 0), width, 50)) for x in flash_centers]\n",
    "    \n",
    "    # Convert flashes to PyTorch tensors and concatenate\n",
    "    xs = [torch.from_numpy(tdrstim.concat(f, nh=filt_depth)) for f in flashes]\n",
    "    \n",
    "    # Return stimuli and velocities\n",
    "    return xs, rightward_moving_bar, c_right, leftward_moving_bar, c_left\n",
    "\n",
    "# Motion reversal\n",
    "def run_motion_reversal(x_locations=np.arange(-9, 3), speed=0.19, clip_n=210, scaling=1):\n",
    "    \"\"\"Gets responses to a bar reversing motion.\"\"\"                        \n",
    "    tflips, Xs = zip(*[s.motion_reversal(xi,speed)[1:] for xi in x_locations])\n",
    "    return tflips, Xs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37598d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'integ_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-918ef6f76238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./analysis_data/{}/motion_anticipation/integrad_r_chan_{}_steps_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_n_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteg_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegrad_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./analysis_data/{}/motion_anticipation/response_r_chan_{}_steps_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_n_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteg_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'integ_steps' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Generate motion anticipation stimuli\n",
    "flash, rightward_moving_bar, c_right, leftward_moving_bar, c_left = motion_anticipation(velocity=0.08, width=6, flash_duration=2, filt_depth=40)\n",
    "\n",
    "# Loop over channels\n",
    "for i in tqdm(range(int(model_n.n_units))):\n",
    "    # Compute integrated gradient and response for rightward moving bar\n",
    "    integrad_r, response_r = tdrutils.integrated_gradient(model=model_n, X=rightward_moving_bar, layer=\"sequential.0\", chans=i, alpha_steps=100)\n",
    "    integrad_r, response_r = integrad_r.numpy(), response_r.numpy()\n",
    "    \n",
    "    # Save results\n",
    "    np.save('./analysis_data/{}/motion_anticipation/integrad_r_chan_{}_steps_{}.npy'.format(model_n_path, i, integ_steps), integrad_r)\n",
    "    np.save('./analysis_data/{}/motion_anticipation/response_r_chan_{}_steps_{}.npy'.format(model_n_path, i, integ_steps), response_r)\n",
    "    \n",
    "    # Compute integrated gradient and response for leftward moving bar\n",
    "    integrad_l, response_l = tdrutils.integrated_gradient(model=model_n, X=leftward_moving_bar, layer=\"sequential.0\", chans=i, alpha_steps=100)\n",
    "    integrad_l, response_l = integrad_l.numpy(), response_l.numpy()\n",
    "    \n",
    "    # Save results\n",
    "    #np.save('./analysis_data/{}/motion_anticipation/integrad_l_chan_{}_steps_{}.npy'.format(model_n_path, i, integ_steps), integrad_l)\n",
    "    #np.save('./analysis_data/{}/motion_anticipation/response_l_chan_{}_steps_{}.npy'.format(model_n_path, i, integ_steps), response_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab483ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
